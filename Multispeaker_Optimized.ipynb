{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5912e289-0c7f-487a-bf98-b8bf78f2ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "import tomli\n",
    "import torch\n",
    "import torchaudio\n",
    "import tqdm\n",
    "\n",
    "from cached_path import cached_path\n",
    "from einops import rearrange\n",
    "from model import CFM, DiT, MMDiT, UNetT\n",
    "from model.utils import (convert_char_to_pinyin, get_tokenizer,\n",
    "                         load_checkpoint, save_spectrogram)\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment, silence\n",
    "from tempfile import NamedTemporaryFile\n",
    "from torchaudio import transforms\n",
    "from transformers import pipeline\n",
    "from vocos import Vocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b0420d2-ecf1-4778-8dac-8927f02b9462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hulk/anaconda3/lib/python3.12/site-packages/vocos/pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "\n",
      "vocab :  Emilia_ZH_EN pinyin\n",
      "tokenizer :  pinyin\n",
      "model :  /home/hulk/.cache/huggingface/hub/models--SWivid--F5-TTS/snapshots/d0ac03c2b5ded76f302ada887ae0da5675e88a5d/F5TTS_Base/model_1200000.safetensors \n",
      "\n",
      "Converting audio...\n",
      "No reference text provided, transcribing reference audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hulk/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished transcription\n",
      "Converting audio...\n",
      "No reference text provided, transcribing reference audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hulk/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished transcription\n",
      "Converting audio...\n",
      "No reference text provided, transcribing reference audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hulk/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished transcription\n",
      "Converting audio...\n",
      "No reference text provided, transcribing reference audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hulk/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished transcription\n",
      "Voice: main\n",
      "A Town Mouse and a Country Mouse were acquaintances, and the Country Mouse one day invited his friend to come and see him at his home in the fields. The Town Mouse came, and they sat down to a dinner of barleycorns and roots, the latter of which had a distinctly earthy flavour. The fare was not much to the taste of the guest, and presently he broke out with\n",
      "ref_text It's not God so much as afterlife. Religion is pretty much about not dying. I would say 99.9% of human beings don't want to die. \n",
      "gen_text 0 A Town Mouse and a Country Mouse were acquaintances,\n",
      "gen_text 1 and the Country Mouse one day invited his friend to come and see him at his home in the fields. The Town Mouse came,\n",
      "gen_text 2 and they sat down to a dinner of barleycorns and roots, the latter of which had a distinctly earthy flavour.\n",
      "gen_text 3 The fare was not much to the taste of the guest, and presently he broke out with\n",
      "Generating audio in 4 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:13<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice: town\n",
      "“My poor dear friend, you live here no better than the ants. Now, you should just see how I fare! My larder is a regular horn of plenty. You must come and stay with me, and I promise you you shall live on the fat of the land.”\n",
      "ref_text The difference in the rainbow depends considerably upon the size of the drops and the width of the coloured band increases as the size of the drops increases. \n",
      "gen_text 0 “My poor dear friend, you live here no better than the ants. Now, you should just see how I fare! My larder is a regular horn of plenty. You must come and stay with me, and I promise you you shall live on the fat of the land.”\n",
      "Generating audio in 1 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice: main\n",
      "So when he returned to town he took the Country Mouse with him, and showed him into a larder containing flour and oatmeal and figs and honey and dates. The Country Mouse had never seen anything like it, and sat down to enjoy the luxuries his friend provided: but before they had well begun, the door of the larder opened and someone came in. The two Mice scampered off and hid themselves in a narrow and exceedingly uncomfortable hole. Presently, when all was quiet, they ventured out again; but someone else came in, and off they scuttled again. This was too much for the visitor.\n",
      "ref_text It's not God so much as afterlife. Religion is pretty much about not dying. I would say 99.9% of human beings don't want to die. \n",
      "gen_text 0 So when he returned to town he took the Country Mouse with him,\n",
      "gen_text 1 and showed him into a larder containing flour and oatmeal and figs and honey and dates.\n",
      "gen_text 2 The Country Mouse had never seen anything like it, and sat down to enjoy the luxuries his friend provided:\n",
      "gen_text 3 but before they had well begun, the door of the larder opened and someone came in.\n",
      "gen_text 4 The two Mice scampered off and hid themselves in a narrow and exceedingly uncomfortable hole. Presently,\n",
      "gen_text 5 when all was quiet, they ventured out again; but someone else came in, and off they scuttled again.\n",
      "gen_text 6 This was too much for the visitor.\n",
      "Generating audio in 7 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:25<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice: country\n",
      "“Goodbye,”\n",
      "ref_text Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. \n",
      "gen_text 0 “Goodbye,”\n",
      "Generating audio in 1 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice: main\n",
      "said he,\n",
      "ref_text It's not God so much as afterlife. Religion is pretty much about not dying. I would say 99.9% of human beings don't want to die. \n",
      "gen_text 0 said he,\n",
      "Generating audio in 1 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice: country\n",
      "“I’m off. You live in the lap of luxury, I can see, but you are surrounded by dangers; whereas at home I can enjoy my simple dinner of roots and corn in peace.\"\n",
      "ref_text Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. \n",
      "gen_text 0 “I’m off. You live in the lap of luxury, I can see, but you are surrounded by dangers; whereas at home I can enjoy my simple dinner of roots and corn in peace.\"\n",
      "Generating audio in 1 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples/out.wav\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "target_rms = 0.1\n",
    "nfe_step = 32\n",
    "cfg_strength = 2.0\n",
    "ode_method = \"euler\"\n",
    "sway_sampling_coef = -1.0\n",
    "speed = 1.0\n",
    "remove_silence_default = True\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Extract necessary configurations from the loaded toml\n",
    "config = tomli.load(open(\"./samples/story.toml\", \"rb\"))\n",
    "\n",
    "ref_audio = config[\"ref_audio\"]\n",
    "ref_text = config[\"ref_text\"]\n",
    "gen_text = config[\"gen_text\"]\n",
    "gen_file = config[\"gen_file\"]\n",
    "if gen_file:\n",
    "    gen_text = codecs.open(gen_file, \"r\", \"utf-8\").read()\n",
    "output_dir = config[\"output_dir\"]\n",
    "model = config[\"model\"]\n",
    "ckpt_file = \"\"\n",
    "vocab_file = \"\"\n",
    "remove_silence = config[\"remove_silence\"]\n",
    "wave_path = Path(output_dir)/\"out.wav\"\n",
    "spectrogram_path = Path(output_dir)/\"out.png\"\n",
    "\n",
    "# Load the vocos model once\n",
    "vocos = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# --------------------- Settings -------------------- #\n",
    "\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "target_rms = 0.1\n",
    "nfe_step = 32  # 16, 32\n",
    "cfg_strength = 2.0\n",
    "ode_method = \"euler\"\n",
    "sway_sampling_coef = -1.0\n",
    "speed = 1.0\n",
    "# fix_duration = 27  # None or float (duration in seconds)\n",
    "fix_duration = None\n",
    "\n",
    "def load_model(model_cls, model_cfg, ckpt_path, file_vocab):\n",
    "    if file_vocab == \"\":\n",
    "        file_vocab = \"Emilia_ZH_EN\"\n",
    "        tokenizer = \"pinyin\"\n",
    "    else:\n",
    "        tokenizer = \"custom\"\n",
    "\n",
    "    print(\"\\nvocab : \", file_vocab, tokenizer)\n",
    "    print(\"tokenizer : \", tokenizer)\n",
    "    print(\"model : \", ckpt_path, \"\\n\")\n",
    "\n",
    "    vocab_char_map, vocab_size = get_tokenizer(file_vocab, tokenizer)\n",
    "    model = CFM(\n",
    "        transformer=model_cls(\n",
    "            **model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels\n",
    "        ),\n",
    "        mel_spec_kwargs=dict(\n",
    "            target_sample_rate=target_sample_rate,\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            hop_length=hop_length,\n",
    "        ),\n",
    "        odeint_kwargs=dict(\n",
    "            method=ode_method,\n",
    "        ),\n",
    "        vocab_char_map=vocab_char_map,\n",
    "    ).to(device)\n",
    "\n",
    "    model = load_checkpoint(model, ckpt_path, device, use_ema=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load models outside of functions to prevent multiple loading\n",
    "F5TTS_model_cfg = dict(\n",
    "    dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4\n",
    ")\n",
    "E2TTS_model_cfg = dict(dim=1024, depth=24, heads=16, ff_mult=4)\n",
    "\n",
    "# Load the TTS model once\n",
    "if model == \"F5-TTS\":\n",
    "    if ckpt_file == \"\":\n",
    "        repo_name = \"F5-TTS\"\n",
    "        exp_name = \"F5TTS_Base\"\n",
    "        ckpt_step = 1200000\n",
    "        ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\n",
    "\n",
    "    ema_model = load_model(DiT, F5TTS_model_cfg, ckpt_file, vocab_file)\n",
    "elif model == \"E2-TTS\":\n",
    "    if ckpt_file == \"\":\n",
    "        repo_name = \"E2-TTS\"\n",
    "        exp_name = \"E2TTS_Base\"\n",
    "        ckpt_step = 1200000\n",
    "        ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\n",
    "\n",
    "    ema_model = load_model(UNetT, E2TTS_model_cfg, ckpt_file, vocab_file)\n",
    "\n",
    "def chunk_text(text, max_chars=135):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks, each with a maximum number of characters.\n",
    "    Args:\n",
    "        text (str): The text to be split.\n",
    "        max_chars (int): The maximum number of characters per chunk.\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    # Split the text into sentences based on punctuation followed by whitespace\n",
    "    sentences = re.split(r'(?<=[;:,.!?])\\s+|(?<=[；：，。！？])', text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk.encode('utf-8')) + len(sentence.encode('utf-8')) <= max_chars:\n",
    "            current_chunk += sentence + \" \" if sentence and len(sentence[-1].encode('utf-8')) == 1 else sentence\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \" if sentence and len(sentence[-1].encode('utf-8')) == 1 else sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def infer_batch(ref_audio, ref_text, gen_text_batches, ema_model, remove_silence, cross_fade_duration=0.15):\n",
    "    audio, sr = ref_audio\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "\n",
    "    rms = torch.sqrt(torch.mean(torch.square(audio)))\n",
    "    if rms < target_rms:\n",
    "        audio = audio * target_rms / rms\n",
    "    if sr != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sample_rate)\n",
    "        audio = resampler(audio)\n",
    "    audio = audio.to(device)\n",
    "\n",
    "    generated_waves = []\n",
    "    spectrograms = []\n",
    "\n",
    "    if len(ref_text[-1].encode('utf-8')) == 1:\n",
    "        ref_text = ref_text + \" \"\n",
    "    for i, gen_text in enumerate(tqdm.tqdm(gen_text_batches)):\n",
    "        # Prepare the text\n",
    "        text_list = [ref_text + gen_text]\n",
    "        final_text_list = convert_char_to_pinyin(text_list)\n",
    "\n",
    "        # Calculate duration\n",
    "        ref_audio_len = audio.shape[-1] // hop_length\n",
    "        zh_pause_punc = r\"。，、；：？！\"\n",
    "        ref_text_len = len(ref_text.encode('utf-8')) + 3 * len(re.findall(zh_pause_punc, ref_text))\n",
    "        gen_text_len = len(gen_text.encode('utf-8')) + 3 * len(re.findall(zh_pause_punc, gen_text))\n",
    "        duration = ref_audio_len + int(ref_audio_len / ref_text_len * gen_text_len / speed)\n",
    "\n",
    "        # inference\n",
    "        with torch.inference_mode():\n",
    "            generated, _ = ema_model.sample(\n",
    "                cond=audio,\n",
    "                text=final_text_list,\n",
    "                duration=duration,\n",
    "                steps=nfe_step,\n",
    "                cfg_strength=cfg_strength,\n",
    "                sway_sampling_coef=sway_sampling_coef,\n",
    "            )\n",
    "\n",
    "        generated = generated[:, ref_audio_len:, :]\n",
    "        generated_mel_spec = rearrange(generated, \"1 n d -> 1 d n\")\n",
    "        generated_wave = vocos.decode(generated_mel_spec.cpu())\n",
    "        if rms < target_rms:\n",
    "            generated_wave = generated_wave * rms / target_rms\n",
    "\n",
    "        # wav -> numpy\n",
    "        generated_wave = generated_wave.squeeze().cpu().numpy()\n",
    "        \n",
    "        generated_waves.append(generated_wave)\n",
    "        spectrograms.append(generated_mel_spec[0].cpu().numpy())\n",
    "\n",
    "    # Combine all generated waves with cross-fading\n",
    "    if cross_fade_duration <= 0:\n",
    "        # Simply concatenate\n",
    "        final_wave = np.concatenate(generated_waves)\n",
    "    else:\n",
    "        final_wave = generated_waves[0]\n",
    "        for i in range(1, len(generated_waves)):\n",
    "            prev_wave = final_wave\n",
    "            next_wave = generated_waves[i]\n",
    "\n",
    "            # Calculate cross-fade samples, ensuring it does not exceed wave lengths\n",
    "            cross_fade_samples = int(cross_fade_duration * target_sample_rate)\n",
    "            cross_fade_samples = min(cross_fade_samples, len(prev_wave), len(next_wave))\n",
    "\n",
    "            if cross_fade_samples <= 0:\n",
    "                # No overlap possible, concatenate\n",
    "                final_wave = np.concatenate([prev_wave, next_wave])\n",
    "                continue\n",
    "\n",
    "            # Overlapping parts\n",
    "            prev_overlap = prev_wave[-cross_fade_samples:]\n",
    "            next_overlap = next_wave[:cross_fade_samples]\n",
    "\n",
    "            # Fade out and fade in\n",
    "            fade_out = np.linspace(1, 0, cross_fade_samples)\n",
    "            fade_in = np.linspace(0, 1, cross_fade_samples)\n",
    "\n",
    "            # Cross-faded overlap\n",
    "            cross_faded_overlap = prev_overlap * fade_out + next_overlap * fade_in\n",
    "\n",
    "            # Combine\n",
    "            new_wave = np.concatenate([\n",
    "                prev_wave[:-cross_fade_samples],\n",
    "                cross_faded_overlap,\n",
    "                next_wave[cross_fade_samples:]\n",
    "            ])\n",
    "\n",
    "            final_wave = new_wave\n",
    "\n",
    "    # Create a combined spectrogram\n",
    "    combined_spectrogram = np.concatenate(spectrograms, axis=1)\n",
    "\n",
    "    return final_wave, combined_spectrogram\n",
    "\n",
    "def process_voice(ref_audio_orig, ref_text):\n",
    "    print(\"Converting audio...\")\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as f:\n",
    "        aseg = AudioSegment.from_file(ref_audio_orig)\n",
    "\n",
    "        non_silent_segs = silence.split_on_silence(aseg, min_silence_len=1000, silence_thresh=-50, keep_silence=1000)\n",
    "        non_silent_wave = AudioSegment.silent(duration=0)\n",
    "        for non_silent_seg in non_silent_segs:\n",
    "            non_silent_wave += non_silent_seg\n",
    "        aseg = non_silent_wave\n",
    "\n",
    "        audio_duration = len(aseg)\n",
    "        if audio_duration > 15000:\n",
    "            print(\"Audio is over 15s, clipping to only first 15s.\")\n",
    "            aseg = aseg[:15000]\n",
    "        aseg.export(f.name, format=\"wav\")\n",
    "        ref_audio = f.name\n",
    "\n",
    "    if not ref_text.strip():\n",
    "        print(\"No reference text provided, transcribing reference audio...\")\n",
    "        pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=\"openai/whisper-large-v3-turbo\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device=device,\n",
    "        )\n",
    "        ref_text = pipe(\n",
    "            ref_audio,\n",
    "            chunk_length_s=30,\n",
    "            batch_size=128,\n",
    "            generate_kwargs={\"task\": \"transcribe\"},\n",
    "            return_timestamps=False,\n",
    "        )[\"text\"].strip()\n",
    "        print(\"Finished transcription\")\n",
    "    else:\n",
    "        print(\"Using custom reference text...\")\n",
    "    return ref_audio, ref_text    \n",
    "\n",
    "def infer(ref_audio, ref_text, gen_text, remove_silence, ema_model, cross_fade_duration=0.15):\n",
    "    print(gen_text)\n",
    "    # Add the functionality to ensure it ends with \". \"\n",
    "    if not ref_text.endswith(\". \") and not ref_text.endswith(\"。\"):\n",
    "        if ref_text.endswith(\".\"):\n",
    "            ref_text += \" \"\n",
    "        else:\n",
    "            ref_text += \". \"\n",
    "\n",
    "    # Split the input text into batches\n",
    "    audio, sr = torchaudio.load(ref_audio)\n",
    "    max_chars = int(len(ref_text.encode('utf-8')) / (audio.shape[-1] / sr) * (25 - audio.shape[-1] / sr))\n",
    "    gen_text_batches = chunk_text(gen_text, max_chars=max_chars)\n",
    "    print('ref_text', ref_text)\n",
    "    for i, gen_text in enumerate(gen_text_batches):\n",
    "        print(f'gen_text {i}', gen_text)\n",
    "    \n",
    "    print(f\"Generating audio in {len(gen_text_batches)} batches...\")\n",
    "    return infer_batch((audio, sr), ref_text, gen_text_batches, ema_model, remove_silence, cross_fade_duration)\n",
    "    \n",
    "\n",
    "def process(ref_audio, ref_text, text_gen, remove_silence, ema_model):\n",
    "    main_voice = {\"ref_audio\": ref_audio, \"ref_text\": ref_text}\n",
    "    if \"voices\" not in config:\n",
    "        voices = {\"main\": main_voice}\n",
    "    else:\n",
    "        voices = config[\"voices\"]\n",
    "        voices[\"main\"] = main_voice\n",
    "    for voice in voices:\n",
    "        voices[voice]['ref_audio'], voices[voice]['ref_text'] = process_voice(voices[voice]['ref_audio'], voices[voice]['ref_text'])\n",
    "\n",
    "    generated_audio_segments = []\n",
    "    reg1 = r'(?=\\[\\w+\\])'\n",
    "    chunks = re.split(reg1, text_gen)\n",
    "    reg2 = r'\\[(\\w+)\\]'\n",
    "    for text in chunks:\n",
    "        match = re.match(reg2, text)\n",
    "        if not match or voice not in voices:\n",
    "            voice = \"main\"\n",
    "        else:\n",
    "            voice = match[1]\n",
    "        text = re.sub(reg2, \"\", text)\n",
    "        gen_text = text.strip()\n",
    "        ref_audio = voices[voice]['ref_audio']\n",
    "        ref_text = voices[voice]['ref_text']\n",
    "        print(f\"Voice: {voice}\")\n",
    "        audio, spectragram = infer(ref_audio, ref_text, gen_text, remove_silence, ema_model)\n",
    "        generated_audio_segments.append(audio)\n",
    "\n",
    "    if generated_audio_segments:\n",
    "        final_wave = np.concatenate(generated_audio_segments)\n",
    "        with open(wave_path, \"wb\") as f:\n",
    "            sf.write(f.name, final_wave, target_sample_rate)\n",
    "            # Remove silence\n",
    "            if remove_silence:\n",
    "                aseg = AudioSegment.from_file(f.name)\n",
    "                non_silent_segs = silence.split_on_silence(aseg, min_silence_len=1000, silence_thresh=-50, keep_silence=500)\n",
    "                non_silent_wave = AudioSegment.silent(duration=0)\n",
    "                for non_silent_seg in non_silent_segs:\n",
    "                    non_silent_wave += non_silent_seg\n",
    "                aseg = non_silent_wave\n",
    "                aseg.export(f.name, format=\"wav\")\n",
    "            print(f.name)\n",
    "\n",
    "# Now call process with the loaded ema_model\n",
    "process(ref_audio, ref_text, gen_text, remove_silence, ema_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83fb6d-263a-4343-9b13-b806282a10ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
